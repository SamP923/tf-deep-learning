{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flower_Classifier_CNNs.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GwUUpK82yC"
      },
      "source": [
        "# Classify Images of Flowers\r\n",
        "**Purpose**:  \r\n",
        "Classify images of flowers with a convolutional neural network using the `tf.keras.Sequential` model and load data using the `ImageDataGenerator` class.\r\n",
        "\r\n",
        "Dataset Used: [Flower dataset from Google](https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz)\r\n",
        "\r\n",
        "\r\n",
        "Project based on [TensorFlow's classification example](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c03_exercise_flowers_with_data_augmentation.ipynb#scrollTo=OYmOylPlVrVt)\r\n",
        "\r\n",
        "<br></br>\r\n",
        "The dataset we have downloaded has following directory structure.\r\n",
        "\r\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\r\n",
        "<b>flower_photos</b>\r\n",
        "|__ <b>daisy</b>\r\n",
        "|__ <b>dandelion</b>\r\n",
        "|__ <b>roses</b>\r\n",
        "|__ <b>sunflowers</b>\r\n",
        "|__ <b>tulips</b>\r\n",
        "</pre>\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ-c7X1n8wFf"
      },
      "source": [
        "# import dependencies \r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import glob\r\n",
        "import shutil\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVmKV9M2-10C"
      },
      "source": [
        "# import tensorflow\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSOwpl4G_JYs"
      },
      "source": [
        "# download and extract contents from the dataset\r\n",
        "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\r\n",
        "\r\n",
        "zip_file = tf.keras.utils.get_file(origin=_URL,\r\n",
        "                                   fname=\"flower_photos.tgz\",\r\n",
        "                                   extract=True)\r\n",
        "\r\n",
        "base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYZraCYZ_g4m"
      },
      "source": [
        "# create labels for the 5 classes\r\n",
        "classes = ['roses', 'daisy', 'dandelion', 'sunflowers', 'tulips']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND0O3sKq_oSb"
      },
      "source": [
        "1. Set up training and validation sets and paths\r\n",
        "2. Apply image augmentation\r\n",
        "3. Create the CNN\r\n",
        "4. Compile the model\r\n",
        "5. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJJydjve_nu1"
      },
      "source": [
        "# create the training and validation sets\r\n",
        "for cl in classes:\r\n",
        "  img_path = os.path.join(base_dir, cl)\r\n",
        "  images = glob.glob(img_path + '/*.jpg')\r\n",
        "  print(\"{}: {} Images\".format(cl, len(images)))\r\n",
        "  train, val = images[:round(len(images)*0.8)], images[round(len(images)*0.8):]\r\n",
        "\r\n",
        "  # make the train/validation directories if they don't exist, then move photo into the new directory\r\n",
        "  for t in train:\r\n",
        "    if not os.path.exists(os.path.join(base_dir, 'train', cl)):\r\n",
        "      os.makedirs(os.path.join(base_dir, 'train', cl))\r\n",
        "    shutil.move(t, os.path.join(base_dir, 'train', cl))\r\n",
        "\r\n",
        "  for v in val:\r\n",
        "    if not os.path.exists(os.path.join(base_dir, 'val', cl)):\r\n",
        "      os.makedirs(os.path.join(base_dir, 'val', cl))\r\n",
        "    shutil.move(v, os.path.join(base_dir, 'val', cl))\r\n",
        "\r\n",
        "  \r\n",
        "# set up path for training/validation\r\n",
        "train_dir = os.path.join(base_dir, 'train')\r\n",
        "val_dir = os.path.join(base_dir, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSfefg3kkNIK"
      },
      "source": [
        "# set model parameters\r\n",
        "BATCH_SIZE = 100\r\n",
        "IMG_SHAPE = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb2-XjYLkUGM"
      },
      "source": [
        "# Image Augmentations\r\n",
        "# rondom rotations, zoom, flip, width and height shifts\r\n",
        "image_gen_train = ImageDataGenerator(\r\n",
        "    rescale=1./255,\r\n",
        "    rotation_range=45,\r\n",
        "    width_shift_range=0.15,\r\n",
        "    height_shift_range=0.15,\r\n",
        "    zoom_range=0.5,\r\n",
        "    horizontal_flip=True,\r\n",
        "    fill_mode='nearest'\r\n",
        ")\r\n",
        "\r\n",
        "train_data_gen = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,\r\n",
        "                                                     directory=train_dir,\r\n",
        "                                                     shuffle=True,\r\n",
        "                                                     target_size=(IMG_SHAPE, IMG_SHAPE),\r\n",
        "                                                     class_mode='sparse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ_lptMhmFDI"
      },
      "source": [
        "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\r\n",
        "def plotImages(images_arr):\r\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\r\n",
        "    axes = axes.flatten()\r\n",
        "    for img, ax in zip( images_arr, axes):\r\n",
        "        ax.imshow(img)\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\r\n",
        "plotImages(augmented_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqvU8vKRmOkg"
      },
      "source": [
        "# create a data geneartor for the validation set\r\n",
        "image_gen_val = ImageDataGenerator(\r\n",
        "    rescale=1./255\r\n",
        ")\r\n",
        "\r\n",
        "val_data_gen = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,\r\n",
        "                                                   directory=val_dir,\r\n",
        "                                                   target_size=(IMG_SHAPE, IMG_SHAPE),\r\n",
        "                                                   class_mode='sparse'\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfnv56KbnaFi"
      },
      "source": [
        "# create the CNN\r\n",
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "\r\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "\r\n",
        "    tf.keras.layers.Dropout(0.2),\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    tf.keras.layers.Dense(512, activation='softmax'),\r\n",
        "\r\n",
        "    tf.keras.layers.Dropout(0.2),\r\n",
        "    tf.keras.layers.Dense(5)\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygdF1sUQoynk"
      },
      "source": [
        "# compile the model\r\n",
        "model.compile(optimizer='adam',\r\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F719c7_rL3J"
      },
      "source": [
        "total_train = len(os.listdir(train_dir))\r\n",
        "total_val = len(os.listdir(val_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtQIYSJRpSYI"
      },
      "source": [
        "# train the model\r\n",
        "epochs = 80\r\n",
        "history = model.fit_generator(\r\n",
        "    train_data_gen,\r\n",
        "    steps_per_epoch = int(np.ceil(total_train / float(BATCH_SIZE))),\r\n",
        "    epochs=epochs,\r\n",
        "    validation_data=val_data_gen,\r\n",
        "    validation_steps=int(np.ceil(total_val / float(BATCH_SIZE)))\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Kq8BvYr-fs"
      },
      "source": [
        "# plot training and validation graphs\r\n",
        "acc = history.history['accuracy']\r\n",
        "val_acc = history.history['val_accuracy']\r\n",
        "\r\n",
        "loss = history.history['loss']\r\n",
        "val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "epochs_range = range(epochs)\r\n",
        "\r\n",
        "plt.figure(figsize=(8, 8))\r\n",
        "plt.subplot(1, 2, 1)\r\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\r\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\r\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMbC4v1vvU1O"
      },
      "source": [
        "# Extension TODO: Experiment with Different Parameters\r\n",
        "\r\n",
        "So far you've created a CNN with 3 convolutional layers and followed by a fully connected layer with 512 units. In the cells below create a new CNN with a different architecture. Feel free to experiment by changing as many parameters as you like. For example, you can add more convolutional layers, or more fully connected layers. You can also experiment with different filter sizes in your convolutional layers, different number of units in your fully connected layers, different dropout rates, etc... You can also experiment by performing image augmentation with more image transformations that we have seen so far. Take a look at the [ImageDataGenerator Documentation](https://keras.io/preprocessing/image/) to see a full list of all the available image transformations. For example, you can add shear transformations, or you can vary the brightness of the images, etc... Experiment as much as you can and compare the accuracy of your various models. Which parameters give you the best result?"
      ]
    }
  ]
}